# Annotated-BERT
This project demonstrates how to pretrain and fine-tune a BERT model using Masked Language Modeling (MLM) and Next Sentence Prediction (NSP) objectives, then adapt it for text classification tasks such as sentiment analysis. The workflow includes data preparation, custom dataset and model classes, training, evaluation, and model saving.
