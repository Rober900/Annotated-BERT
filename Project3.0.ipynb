{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMmHNEbfVmqA"
      },
      "source": [
        "# Models with MLM y NSP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExRREt3SVcHG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer\n",
        "import random\n",
        "import numpy as np\n",
        "import kagglehub as kh\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Classes to prepare the dataset of both MLM and NSP\n",
        "class BertPretrainingDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length=512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.texts = self._prepare_texts(texts)\n",
        "\n",
        "    def _prepare_texts(self, texts):\n",
        "        # Split texts into sentences and create pairs\n",
        "        sentence_pairs = []\n",
        "        for i in range(len(texts)-1):\n",
        "            # 50% chance of getting actual next sentence vs random sentence\n",
        "            if random.random() < 0.5:\n",
        "                next_sent_idx = i + 1\n",
        "                is_next = 1\n",
        "            else:\n",
        "                next_sent_idx = random.randint(0, len(texts)-1)\n",
        "                is_next = 0\n",
        "\n",
        "            sentence_pairs.append({\n",
        "                'sent1': texts[i],\n",
        "                'sent2': texts[next_sent_idx],\n",
        "                'is_next': is_next\n",
        "            })\n",
        "        return sentence_pairs\n",
        "\n",
        "    def _apply_mlm(self, tokens):\n",
        "        mlm_positions = []\n",
        "        mlm_labels = []\n",
        "\n",
        "        # Select 15% of tokens randomly for MLM\n",
        "        n_tokens = len(tokens)\n",
        "        n_mask = max(1, int(0.15 * n_tokens))\n",
        "        mask_candidates = list(range(n_tokens))\n",
        "        random.shuffle(mask_candidates)\n",
        "        mask_positions = sorted(mask_candidates[:n_mask])\n",
        "\n",
        "        for pos in mask_positions:\n",
        "            mlm_positions.append(pos)\n",
        "            mlm_labels.append(tokens[pos])\n",
        "\n",
        "            prob = random.random()\n",
        "            if prob < 0.8:  # 80% replace with [MASK]\n",
        "                tokens[pos] = self.tokenizer.mask_token_id\n",
        "            elif prob < 0.9:  # 10% replace with random token\n",
        "                tokens[pos] = random.randint(0, self.tokenizer.vocab_size - 1)\n",
        "            # 10% keep unchanged\n",
        "\n",
        "        return tokens, mlm_positions, mlm_labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pair = self.texts[idx]\n",
        "\n",
        "        # Tokenize sentences\n",
        "        encoding = self.tokenizer(\n",
        "            pair['sent1'],\n",
        "            pair['sent2'],\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'].squeeze()\n",
        "        attention_mask = encoding['attention_mask'].squeeze()\n",
        "\n",
        "        # Apply MLM\n",
        "        masked_input_ids, mlm_positions, mlm_labels = self._apply_mlm(input_ids.clone().tolist())\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(masked_input_ids),\n",
        "            'attention_mask': attention_mask,\n",
        "            'mlm_positions': torch.tensor(mlm_positions),\n",
        "            'mlm_labels': torch.tensor(mlm_labels),\n",
        "            'nsp_label': torch.tensor(pair['is_next'])\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "class TextClassificationDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'labels': torch.tensor(self.labels[idx])\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "# Class for the pretrain model, with both MLM and NSP\n",
        "class BertPretrainingModel(nn.Module):\n",
        "    def __init__(self, bert_model):\n",
        "        super().__init__()\n",
        "        self.bert = bert_model\n",
        "        self.config = bert_model.config\n",
        "        self.mlm_head = nn.Linear(bert_model.config.hidden_size, bert_model.config.vocab_size)\n",
        "        self.nsp_head = nn.Linear(bert_model.config.hidden_size, 2)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, mlm_positions=None):\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            return_dict=True\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs.last_hidden_state\n",
        "        pooled_output = outputs.pooler_output\n",
        "\n",
        "        # MLM prediction\n",
        "        if mlm_positions is not None:\n",
        "            mlm_output = sequence_output[torch.arange(sequence_output.size(0)).unsqueeze(1), mlm_positions]\n",
        "            prediction_scores = self.mlm_head(mlm_output)\n",
        "        else:\n",
        "            prediction_scores = self.mlm_head(sequence_output)\n",
        "\n",
        "        # NSP prediction\n",
        "        seq_relationship_score = self.nsp_head(pooled_output)\n",
        "\n",
        "        return prediction_scores, seq_relationship_score\n",
        "\n",
        "class BertForTextClassification(nn.Module):\n",
        "    def __init__(self, bert_model, num_classes):\n",
        "        super().__init__()\n",
        "        self.bert = bert_model\n",
        "        self.classifier = nn.Linear(bert_model.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            return_dict=True\n",
        "        )\n",
        "        pooled_output = outputs.pooler_output  # CLS token\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StgBvviDVsPR"
      },
      "source": [
        "# Function to pretrain BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IWcwgcxVekh"
      },
      "outputs": [],
      "source": [
        "# Function to pretrain x epochs and tell the loss\n",
        "def pretrain_bert(model, train_dataloader, optimizer, num_epochs):\n",
        "    model.train()\n",
        "    mlm_criterion = nn.CrossEntropyLoss()\n",
        "    nsp_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # MLM positions and lable is the information to know the MASKED tokens.\n",
        "    # NSP label is to know if the following sentence is the next sentence or not\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for batch in train_dataloader:\n",
        "            input_ids = batch['input_ids']\n",
        "            attention_mask = batch['attention_mask']\n",
        "            mlm_positions = batch['mlm_positions']\n",
        "            mlm_labels = batch['mlm_labels']\n",
        "            nsp_labels = batch['nsp_label']\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            mlm_scores, nsp_scores = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                mlm_positions=mlm_positions\n",
        "            )\n",
        "\n",
        "            # Calculate MLM loss\n",
        "            mlm_loss = mlm_criterion(mlm_scores.view(-1, model.bert.config.vocab_size), mlm_labels.view(-1))\n",
        "\n",
        "            # Calculate NSP loss\n",
        "            nsp_loss = nsp_criterion(nsp_scores, nsp_labels)\n",
        "\n",
        "            # Combined loss\n",
        "            loss = mlm_loss + nsp_loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_dataloader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4gS372mgL3B"
      },
      "source": [
        "# Fine tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PiwUV_tgK6f"
      },
      "outputs": [],
      "source": [
        "def fine_tune_bert(model, dataloader, optimizer, num_epochs):\n",
        "    model.train()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # The final label is to predict the sentiment of the review\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids']\n",
        "            attention_mask = batch['attention_mask']\n",
        "            labels = batch['labels']\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BIhrESPVwfV"
      },
      "source": [
        "# Get data from the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yi7MlYKAVhdB"
      },
      "outputs": [],
      "source": [
        "def read_file(file_path):\n",
        "  with open(file_path, 'r', encoding='UTF-8') as f:\n",
        "    text=[]\n",
        "    for line in f:\n",
        "      text.append(line.replace('\\n',''))\n",
        "  return text\n",
        "\n",
        "file_contents = read_file('./sample_text.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NvIFL0PViTc"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "# Initialize tokenizer and base model for file\n",
        "tokenizer = BertTokenizer.from_pretrained('prajjwal1/bert-tiny')\n",
        "base_model = BertModel.from_pretrained('prajjwal1/bert-tiny')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrNcPgIRVjMp"
      },
      "outputs": [],
      "source": [
        "# Create dataset\n",
        "dataset = BertPretrainingDataset(file_contents, tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_53OOCPaVkdr"
      },
      "outputs": [],
      "source": [
        "# Initialize model and optimizer\n",
        "model = BertPretrainingModel(base_model)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WzcWLxlVlaO",
        "outputId": "0c9bcbd8-185d-4779-b45a-38839a873243"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Average Loss: 10.5509\n",
            "Epoch 2/10, Average Loss: 9.1859\n",
            "Epoch 3/10, Average Loss: 8.0982\n",
            "Epoch 4/10, Average Loss: 7.2646\n",
            "Epoch 5/10, Average Loss: 6.6895\n",
            "Epoch 6/10, Average Loss: 6.2117\n",
            "Epoch 7/10, Average Loss: 5.7615\n",
            "Epoch 8/10, Average Loss: 5.5444\n",
            "Epoch 9/10, Average Loss: 5.3004\n",
            "Epoch 10/10, Average Loss: 5.1597\n"
          ]
        }
      ],
      "source": [
        "# Start pretraining\n",
        "pretrain_bert(model, dataloader, optimizer, num_epochs=10)\n",
        "pretrained_model_state = model.state_dict() # <- Guardas en esa variable el modelo preentrenado\n",
        "torch.save(pretrained_model_state, 'pretrained_bert_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3F6CqL3hKYC"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Clean the text by removing html tags\n",
        "    \"\"\"\n",
        "\n",
        "    text = re.sub(r\"<br\\s*/?>\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0F3af5HZg48w"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "path = kh.dataset_download(\"mahmoudshaheen1134/imdp-data\")\n",
        "\n",
        "full_path = os.path.join(path, os.listdir(path)[0])\n",
        "data = pd.read_csv(full_path)\n",
        "data = data.dropna()\n",
        "data = data.drop_duplicates()\n",
        "\n",
        "data['review'] = data['review'].apply(clean_text)\n",
        "data['sentiment'] = data['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
        "\n",
        "# dataset = TextClassificationDataset(data['review'].tolist(), data['sentiment'].tolist(), tokenizer)\n",
        "# dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# num_classes = 2\n",
        "# model = BertForTextClassification(model, num_classes)\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "# fine_tune_bert(model, dataloader, optimizer, num_epochs=10)\n",
        "data['review']\n",
        "# Split the data into training and validation sets\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    data['review'], data['sentiment'], test_size=0.95, random_state=42\n",
        ")\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = TextClassificationDataset(train_texts.tolist(), train_labels.tolist(), tokenizer)\n",
        "val_dataset = TextClassificationDataset(val_texts.tolist(), val_labels.tolist(), tokenizer)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNvcfZwUZbUo",
        "outputId": "0d693f48-7f44-45f4-a357-84778a041fe9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Average Loss: 0.6638\n",
            "Epoch 2/10, Average Loss: 0.6277\n",
            "Epoch 3/10, Average Loss: 0.5851\n",
            "Epoch 4/10, Average Loss: 0.5358\n",
            "Epoch 5/10, Average Loss: 0.4886\n",
            "Epoch 6/10, Average Loss: 0.4383\n",
            "Epoch 7/10, Average Loss: 0.3944\n",
            "Epoch 8/10, Average Loss: 0.3543\n",
            "Epoch 9/10, Average Loss: 0.3258\n",
            "Epoch 10/10, Average Loss: 0.2839\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import random\n",
        "\n",
        "# Modify BertPretrainingModel for classification\n",
        "class BertClassificationModel(nn.Module):\n",
        "    def __init__(self, bert_model, num_classes):\n",
        "        super().__init__()\n",
        "        self.bert = bert_model\n",
        "        self.config = bert_model.config\n",
        "        self.classification_head = nn.Linear(bert_model.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Get BERT outputs\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
        "        pooled_output = outputs.pooler_output\n",
        "\n",
        "        # Classification head\n",
        "        logits = self.classification_head(pooled_output)\n",
        "        return logits\n",
        "\n",
        "# Example Classification Dataset\n",
        "class ClassificationDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'].squeeze()\n",
        "        attention_mask = encoding['attention_mask'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "# Training loop for fine-tuning\n",
        "def fine_tune_bert(model, train_dataloader, optimizer, num_epochs):\n",
        "    model.train()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for batch in train_dataloader:\n",
        "            input_ids = batch['input_ids']\n",
        "            attention_mask = batch['attention_mask']\n",
        "            labels = batch['labels']\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_dataloader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Example Classification Dataset (You will use your own data here)\n",
        "# texts = [\"This is a positive example.\", \"This is a negative example.\"]\n",
        "# labels = [1, 0]  # 1 for positive, 0 for negative\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('prajjwal1/bert-tiny')\n",
        "base_model = BertModel.from_pretrained('prajjwal1/bert-tiny')\n",
        "\n",
        "# Create classification dataset\n",
        "# dataset = ClassificationDataset(texts, labels, tokenizer)\n",
        "# dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# Initialize the fine-tuning model\n",
        "num_classes = 2  # For binary classification (adjust as needed)\n",
        "model = BertClassificationModel(base_model, num_classes)\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Fine-tune the model\n",
        "fine_tune_bert(model, train_dataloader, optimizer, num_epochs=10)\n",
        "\n",
        "# Save the fine-tuned model\n",
        "torch.save(model.state_dict(), 'fine_tuned_bert_model.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPnnOBYzZbUp",
        "outputId": "9a071b71-a67c-40bc-fd13-5c5942c4e288"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0% has been processed\n",
            "10% has been processed\n",
            "20% has been processed\n",
            "30% has been processed\n",
            "40% has been processed\n",
            "50% has been processed\n",
            "60% has been processed\n",
            "70% has been processed\n",
            "80% has been processed\n",
            "90% has been processed\n",
            "100% has been processed\n",
            "Validation Accuracy: 0.8191\n"
          ]
        }
      ],
      "source": [
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        total_batches = len(dataloader)\n",
        "        for i, batch in enumerate(dataloader):\n",
        "            if i % (total_batches // 10) == 0:\n",
        "                print(f'{(i / total_batches) * 100:.0f}% has been processed')\n",
        "            input_ids = batch['input_ids']\n",
        "            attention_mask = batch['attention_mask']\n",
        "            labels = batch['labels']\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            _, predicted_class = torch.max(outputs, dim=1)\n",
        "\n",
        "            correct_predictions += (predicted_class == labels).sum().item()\n",
        "            total_predictions += labels.size(0)\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    return accuracy\n",
        "\n",
        "# Evaluate the model on the validation dataset\n",
        "accuracy = evaluate_model(model, val_dataloader)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "project",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}